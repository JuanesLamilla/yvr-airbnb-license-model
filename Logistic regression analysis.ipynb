{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "listings_df = pd.read_csv(\"data/yvr_listing_data_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Deal with multicollinearity: Delete variables with highest VIF (From QM Practical 4)\n",
    "\n",
    "# calculating VIF\n",
    "# This function is adjusted from: https://stackoverflow.com/a/51329496/4667568\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "def drop_column_using_vif_(df, thresh=3):\n",
    "    '''\n",
    "    Calculates VIF each feature in a pandas dataframe, and repeatedly drop the columns with the highest VIF\n",
    "    A constant must be added to variance_inflation_factor or the results will be incorrect\n",
    "\n",
    "    :param df: the pandas dataframe containing only the predictor features, not the response variable\n",
    "    :param thresh: (default 5) the threshould VIF value. If the VIF of a variable is greater than thresh, it should be removed from the dataframe\n",
    "    :return: dataframe with multicollinear features removed\n",
    "    '''\n",
    "    while True:\n",
    "        # adding a constatnt item to the data. add_constant is a function from statsmodels (see the import above)\n",
    "        df_with_const = add_constant(df)\n",
    "\n",
    "        vif_df = pd.Series([variance_inflation_factor(df_with_const.values, i) \n",
    "               for i in range(df_with_const.shape[1])], name= \"VIF\",\n",
    "              index=df_with_const.columns).to_frame()\n",
    "\n",
    "        # drop the const\n",
    "        vif_df = vif_df.drop('const')\n",
    "        \n",
    "        # if the largest VIF is above the thresh, remove a variable with the largest VIF\n",
    "        # If there are multiple variabels with VIF>thresh, only one of them is removed. This is because we want to keep as many variables as possible\n",
    "        if vif_df.VIF.max() > thresh:\n",
    "            # If there are multiple variables with the maximum VIF, choose the first one\n",
    "            index_to_drop = vif_df.index[vif_df.VIF == vif_df.VIF.max()].tolist()[0]\n",
    "            print('Dropping: {}'.format(index_to_drop))\n",
    "            df = df.drop(columns = index_to_drop)\n",
    "        else:\n",
    "            # No VIF is above threshold. Exit the loop\n",
    "            break\n",
    "\n",
    "    return df\n",
    "\n",
    "df_predictors_selected_VIF = drop_column_using_vif_(listings_df, threshhold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create logistic regression model (From Juanes)\n",
    "\n",
    "\"\"\"\n",
    "Create a multiple logistic regression model to predict legal_listing using the following independent variables:\n",
    "review_scores_rating, price, instant_bookable\n",
    "\"\"\"\n",
    "# Convert instant_bookable to a boolean variable\n",
    "listings_df['instant_bookable'] = listings_df['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "# Convert price to a float variable\n",
    "listings_df['price'] = listings_df['price'].str.replace('$', '').str.replace(',', '').astype(float)\n",
    "\n",
    "# Prepare the data\n",
    "X = listings_df[['review_scores_rating', 'price', 'instant_bookable']]\n",
    "y = listings_df['legal_listing']\n",
    "\n",
    "# Add constant to the independent variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = sm.Logit(y, X)\n",
    "result = model.fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(result.summary())\n",
    "\n",
    "# Using the results of a logistic regression model, predict the probability of a listing being legal\n",
    "# given the following values of the independent variables:\n",
    "# review_scores_rating = 4.5, price = $256, instant_bookable = False\n",
    "print(\"\\n       Prediction Results\")\n",
    "print(\"Probability of being legal:\", result.predict([1, 4.5, 256, 0])[0])\n",
    "\n",
    "# Show formula for the model\n",
    "print(\"\\n       Model Formula\")\n",
    "print(\"logit(p) = \", result.params[0], \"+\", result.params[1], \"* review_scores_rating +\", result.params[2], \"* price +\", result.params[3], \"* instant_bookable\")\n",
    "\n",
    "# Apply formula to calculate the probability of being legal (sigmoid function)\n",
    "print(\"\\n       Prediction Results\")\n",
    "print(\"Probability of being legal:\", 1 / (1 + 2.71828 ** -(result.params[0] + result.params[1] * 4.5 + result.params[2] * 256 + result.params[3] * 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interpret the logistic regression model\n",
    "\n",
    "## 1. Model fitness: R-squre, adjusted R-squre, p-value (From Juanes)\n",
    "\"\"\"\n",
    "Using the logistic regression Wald test, see if there is a connection between review_scores_rating (independent variable)\n",
    "and legal_listing (dependent variable). The null hypothesis is that there is no connection between the two variables.\n",
    "review_scores_rating is a continuous variable from 0 to 5, while legal_listing is a categorial boolean variable.\n",
    "\"\"\"\n",
    "# Remove rows where review_scores_rating is NaN\n",
    "listings_df = listings_df[listings_df['review_scores_rating'].notna()]\n",
    "\n",
    "# Prepare the data\n",
    "X = listings_df['review_scores_rating']\n",
    "y = listings_df['legal_listing']\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = sm.Logit(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "\n",
    "print(result.summary())\n",
    "\n",
    "# Perform the Wald test\n",
    "wald_test = result.wald_test(\"review_scores_rating = 0\", scalar=True)\n",
    "\n",
    "# Print the Wald test results\n",
    "print(\"\\n       Wald Test Results\")\n",
    "print(\"Test Statistic:\", wald_test.statistic)\n",
    "print(\"P-value:\", wald_test.pvalue)\n",
    "print(\"Degrees of Freedom:\", wald_test.df_denom)\n",
    "\n",
    "## or 1. Model fitness: R-squre, adjusted R-squre, p-value (From QM Practical 4)\n",
    "model_listings_df = sm.OLS(endog=listing_df[['review_scores_rating']], exog=sm.add_constant(df_predictors_selected_VIF)).fit()\n",
    "model_listings_df.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interpret the logistic regression model (From QM Practical 4)\n",
    "## 2. Residual analysis: Linear relationship: rainbow test\n",
    "#                      Independent values: Durbin-Watson test\n",
    "#                      normally distributed values: Jarque Bera test, \n",
    "#                      equal variance: Goldfeld-Quandt homoskedasticity test\n",
    "# As each of the p-value is less than 0.05, we can reject null hypothesis\n",
    "\n",
    "# Linear relationship: rainbow test\n",
    "test_rainbow = statsmodels.stats.diagnostic.linear_rainbow(model_listings_df)\n",
    "# This function returns a tuple consisting of two values: the test statistic based on the F test and the pvalue of the test\n",
    "# Note that these two values are not named. Therefore, you need to know the order before accessing these two values.\n",
    "print(\"The p value of the rainbow test: {:.4f}\".format(test_rainbow[1]))\n",
    "\n",
    "#Independent values: Durbin-Watson test\n",
    "test_dw = statsmodels.stats.stattools.durbin_watson(model_listings_df.resid)\n",
    "print(\"Durbin-Watson test statistic is: {:.4f}\".format(test_dw))\n",
    "\n",
    "#normally distributed values: Jarque Bera test\n",
    "test_JB = statsmodels.stats.stattools.jarque_bera(model_listings_df.resid)\n",
    "print(\"The p value of the Jarque Bera test: {:.4f}\".format(test_JB[1]))\n",
    "\n",
    "#Equal variance: Goldfeld-Quandt homoskedasticity test\n",
    "statsmodels.stats.diagnostic.het_goldfeldquandt(model_listings_df.model.endog, model_listings_df.model.exog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interpret the logistic regression model\n",
    "## 3. Coefficients\n",
    "# As in the third cell \"Create logistic regression model (From Juanes)\".\n",
    "# Using coefficients to evaluate each variable's weight, direction, statistical significance, Variable Standardization when we trying to interpret the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
