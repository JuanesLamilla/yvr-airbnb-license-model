{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "listings_df = pd.read_csv(os.path.join('data', 'yvr_listing_data_cleaned.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nvariables_significance_info = pd.DataFrame()\\n\\nvariables_significance_info['id']=pd.Series(dtype=int)\\nvariables_significance_info['variable']=pd.Series(dtype=str)\\nvariables_significance_info['P-value']=pd.Series(dtype=float)\\nvariables_significance_info['selection'] = pd.Series(dtype=bool)\\nvariables_significance_info.to_csv(os.path.join('data', 'variables_sigificance_info.csv'), index=False)\\n\\ntype(variables_significance_info)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new csv file to storage correlation information for all possible variables\n",
    "\"\"\"\n",
    "variables_significance_info = pd.DataFrame()\n",
    "\n",
    "variables_significance_info['id']=pd.Series(dtype=int)\n",
    "variables_significance_info['variable']=pd.Series(dtype=str)\n",
    "variables_significance_info['P-value']=pd.Series(dtype=float)\n",
    "variables_significance_info['selection'] = pd.Series(dtype=bool)\n",
    "variables_significance_info.to_csv(os.path.join('data', 'variables_sigificance_info.csv'), index=False)\n",
    "\n",
    "type(variables_significance_info)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review_scores_rating**\n",
    "\n",
    "**P-value** = [**]      [<0.05 / >0.05]\n",
    "\n",
    "**Selection** =[True/False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat in module pandas.core.reshape.concat:\n",
      "\n",
      "concat(objs: 'Iterable[NDFrame] | Mapping[HashableT, NDFrame]', *, axis: 'Axis' = 0, join: 'str' = 'outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, copy: 'bool | None' = None) -> 'DataFrame | Series'\n",
      "    Concatenate pandas objects along a particular axis.\n",
      "    \n",
      "    Allows optional set logic along the other axes.\n",
      "    \n",
      "    Can also add a layer of hierarchical indexing on the concatenation axis,\n",
      "    which may be useful if the labels are the same (or overlapping) on\n",
      "    the passed axis number.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    objs : a sequence or mapping of Series or DataFrame objects\n",
      "        If a mapping is passed, the sorted keys will be used as the `keys`\n",
      "        argument, unless it is passed, in which case the values will be\n",
      "        selected (see below). Any None objects will be dropped silently unless\n",
      "        they are all None in which case a ValueError will be raised.\n",
      "    axis : {0/'index', 1/'columns'}, default 0\n",
      "        The axis to concatenate along.\n",
      "    join : {'inner', 'outer'}, default 'outer'\n",
      "        How to handle indexes on other axis (or axes).\n",
      "    ignore_index : bool, default False\n",
      "        If True, do not use the index values along the concatenation axis. The\n",
      "        resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n",
      "        concatenating objects where the concatenation axis does not have\n",
      "        meaningful indexing information. Note the index values on the other\n",
      "        axes are still respected in the join.\n",
      "    keys : sequence, default None\n",
      "        If multiple levels passed, should contain tuples. Construct\n",
      "        hierarchical index using the passed keys as the outermost level.\n",
      "    levels : list of sequences, default None\n",
      "        Specific levels (unique values) to use for constructing a\n",
      "        MultiIndex. Otherwise they will be inferred from the keys.\n",
      "    names : list, default None\n",
      "        Names for the levels in the resulting hierarchical index.\n",
      "    verify_integrity : bool, default False\n",
      "        Check whether the new concatenated axis contains duplicates. This can\n",
      "        be very expensive relative to the actual data concatenation.\n",
      "    sort : bool, default False\n",
      "        Sort non-concatenation axis if it is not already aligned.\n",
      "    \n",
      "    copy : bool, default True\n",
      "        If False, do not copy data unnecessarily.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    object, type of objs\n",
      "        When concatenating all ``Series`` along the index (axis=0), a\n",
      "        ``Series`` is returned. When ``objs`` contains at least one\n",
      "        ``DataFrame``, a ``DataFrame`` is returned. When concatenating along\n",
      "        the columns (axis=1), a ``DataFrame`` is returned.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.join : Join DataFrames using indexes.\n",
      "    DataFrame.merge : Merge DataFrames by indexes or columns.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The keys, levels, and names arguments are all optional.\n",
      "    \n",
      "    A walkthrough of how this method fits in with other tools for combining\n",
      "    pandas objects can be found `here\n",
      "    <https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html>`__.\n",
      "    \n",
      "    It is not recommended to build DataFrames by adding single rows in a\n",
      "    for loop. Build a list of rows and make a DataFrame in a single concat.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Combine two ``Series``.\n",
      "    \n",
      "    >>> s1 = pd.Series(['a', 'b'])\n",
      "    >>> s2 = pd.Series(['c', 'd'])\n",
      "    >>> pd.concat([s1, s2])\n",
      "    0    a\n",
      "    1    b\n",
      "    0    c\n",
      "    1    d\n",
      "    dtype: object\n",
      "    \n",
      "    Clear the existing index and reset it in the result\n",
      "    by setting the ``ignore_index`` option to ``True``.\n",
      "    \n",
      "    >>> pd.concat([s1, s2], ignore_index=True)\n",
      "    0    a\n",
      "    1    b\n",
      "    2    c\n",
      "    3    d\n",
      "    dtype: object\n",
      "    \n",
      "    Add a hierarchical index at the outermost level of\n",
      "    the data with the ``keys`` option.\n",
      "    \n",
      "    >>> pd.concat([s1, s2], keys=['s1', 's2'])\n",
      "    s1  0    a\n",
      "        1    b\n",
      "    s2  0    c\n",
      "        1    d\n",
      "    dtype: object\n",
      "    \n",
      "    Label the index keys you create with the ``names`` option.\n",
      "    \n",
      "    >>> pd.concat([s1, s2], keys=['s1', 's2'],\n",
      "    ...           names=['Series name', 'Row ID'])\n",
      "    Series name  Row ID\n",
      "    s1           0         a\n",
      "                 1         b\n",
      "    s2           0         c\n",
      "                 1         d\n",
      "    dtype: object\n",
      "    \n",
      "    Combine two ``DataFrame`` objects with identical columns.\n",
      "    \n",
      "    >>> df1 = pd.DataFrame([['a', 1], ['b', 2]],\n",
      "    ...                    columns=['letter', 'number'])\n",
      "    >>> df1\n",
      "      letter  number\n",
      "    0      a       1\n",
      "    1      b       2\n",
      "    >>> df2 = pd.DataFrame([['c', 3], ['d', 4]],\n",
      "    ...                    columns=['letter', 'number'])\n",
      "    >>> df2\n",
      "      letter  number\n",
      "    0      c       3\n",
      "    1      d       4\n",
      "    >>> pd.concat([df1, df2])\n",
      "      letter  number\n",
      "    0      a       1\n",
      "    1      b       2\n",
      "    0      c       3\n",
      "    1      d       4\n",
      "    \n",
      "    Combine ``DataFrame`` objects with overlapping columns\n",
      "    and return everything. Columns outside the intersection will\n",
      "    be filled with ``NaN`` values.\n",
      "    \n",
      "    >>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n",
      "    ...                    columns=['letter', 'number', 'animal'])\n",
      "    >>> df3\n",
      "      letter  number animal\n",
      "    0      c       3    cat\n",
      "    1      d       4    dog\n",
      "    >>> pd.concat([df1, df3], sort=False)\n",
      "      letter  number animal\n",
      "    0      a       1    NaN\n",
      "    1      b       2    NaN\n",
      "    0      c       3    cat\n",
      "    1      d       4    dog\n",
      "    \n",
      "    Combine ``DataFrame`` objects with overlapping columns\n",
      "    and return only those that are shared by passing ``inner`` to\n",
      "    the ``join`` keyword argument.\n",
      "    \n",
      "    >>> pd.concat([df1, df3], join=\"inner\")\n",
      "      letter  number\n",
      "    0      a       1\n",
      "    1      b       2\n",
      "    0      c       3\n",
      "    1      d       4\n",
      "    \n",
      "    Combine ``DataFrame`` objects horizontally along the x axis by\n",
      "    passing in ``axis=1``.\n",
      "    \n",
      "    >>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n",
      "    ...                    columns=['animal', 'name'])\n",
      "    >>> pd.concat([df1, df4], axis=1)\n",
      "      letter  number  animal    name\n",
      "    0      a       1    bird   polly\n",
      "    1      b       2  monkey  george\n",
      "    \n",
      "    Prevent the result from including duplicate index values with the\n",
      "    ``verify_integrity`` option.\n",
      "    \n",
      "    >>> df5 = pd.DataFrame([1], index=['a'])\n",
      "    >>> df5\n",
      "       0\n",
      "    a  1\n",
      "    >>> df6 = pd.DataFrame([2], index=['a'])\n",
      "    >>> df6\n",
      "       0\n",
      "    a  2\n",
      "    >>> pd.concat([df5, df6], verify_integrity=True)\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    ValueError: Indexes have overlapping values: ['a']\n",
      "    \n",
      "    Append a single row to the end of a ``DataFrame`` object.\n",
      "    \n",
      "    >>> df7 = pd.DataFrame({'a': 1, 'b': 2}, index=[0])\n",
      "    >>> df7\n",
      "        a   b\n",
      "    0   1   2\n",
      "    >>> new_row = pd.Series({'a': 3, 'b': 4})\n",
      "    >>> new_row\n",
      "    a    3\n",
      "    b    4\n",
      "    dtype: int64\n",
      "    >>> pd.concat([df7, new_row.to_frame().T], ignore_index=True)\n",
      "        a   b\n",
      "    0   1   2\n",
      "    1   3   4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.563565\n",
      "         Iterations 5\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          legal_listing   No. Observations:                 5641\n",
      "Model:                          Logit   Df Residuals:                     5639\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Fri, 24 Nov 2023   Pseudo R-squ.:                0.003890\n",
      "Time:                        13:57:58   Log-Likelihood:                -3179.1\n",
      "converged:                       True   LL-Null:                       -3191.5\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.257e-07\n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "const                   -0.4340      0.302     -1.437      0.151      -1.026       0.158\n",
      "review_scores_rating     0.3192      0.063      5.035      0.000       0.195       0.443\n",
      "========================================================================================\n",
      "\n",
      "       Wald Test Results\n",
      "Test Statistic: 25.353652870050457\n",
      "P-value: 4.772477793182879e-07\n",
      "Degrees of Freedom: 1.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type '<class 'str'>'; only Series and DataFrame objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\SBH\\OneDrive - University College London\\#CASA0007__QuantitativeMethods\\ASSESSMENT\\GroupworkPresentation\\yvr-airbnb-license-model\\Selecting_Variables_Correlation_with_Licence.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SBH/OneDrive%20-%20University%20College%20London/%23CASA0007__QuantitativeMethods/ASSESSMENT/GroupworkPresentation/yvr-airbnb-license-model/Selecting_Variables_Correlation_with_Licence.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m variables_significance_info \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvariables_sigificance_info.csv\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SBH/OneDrive%20-%20University%20College%20London/%23CASA0007__QuantitativeMethods/ASSESSMENT/GroupworkPresentation/yvr-airbnb-license-model/Selecting_Variables_Correlation_with_Licence.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m new_row \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mreview_scores_rating\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m4.772477793182879e-07\u001b[39m, \u001b[39mTrue\u001b[39;00m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SBH/OneDrive%20-%20University%20College%20London/%23CASA0007__QuantitativeMethods/ASSESSMENT/GroupworkPresentation/yvr-airbnb-license-model/Selecting_Variables_Correlation_with_Licence.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m variables_significance_info \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(new_row, ignore_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SBH/OneDrive%20-%20University%20College%20London/%23CASA0007__QuantitativeMethods/ASSESSMENT/GroupworkPresentation/yvr-airbnb-license-model/Selecting_Variables_Correlation_with_Licence.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m variables_significance_info\u001b[39m.\u001b[39mto_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvariables_sigificance_info.csv\u001b[39m\u001b[39m'\u001b[39m), index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:372\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39melif\u001b[39;00m copy \u001b[39mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    370\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    373\u001b[0m     objs,\n\u001b[0;32m    374\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    375\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m    376\u001b[0m     join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m    377\u001b[0m     keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[0;32m    378\u001b[0m     levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[0;32m    379\u001b[0m     names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    380\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m    381\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    382\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    383\u001b[0m )\n\u001b[0;32m    385\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:462\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, (ABCSeries, ABCDataFrame)):\n\u001b[0;32m    458\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    459\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcannot concatenate object of type \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(obj)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    460\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39monly Series and DataFrame objs are valid\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m         )\n\u001b[1;32m--> 462\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m    464\u001b[0m     ndims\u001b[39m.\u001b[39madd(obj\u001b[39m.\u001b[39mndim)\n\u001b[0;32m    466\u001b[0m \u001b[39m# get the sample\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39m# want the highest ndim that we have, and must be non-empty\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m# unless all objs are empty\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'str'>'; only Series and DataFrame objs are valid"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using the logistic regression Wald test, see if there is a connection between review_scores_rating (independent variable)\n",
    "and legal_listing (dependent variable). The null hypothesis is that there is no connection between the two variables.\n",
    "review_scores_rating is a continuous variable from 0 to 5, while legal_listing is a categorical boolean variable.\n",
    "\"\"\"\n",
    "# Remove rows where review_scores_rating is NaN\n",
    "listings_df = listings_df[listings_df['review_scores_rating'].notna()]\n",
    "\n",
    "# Prepare the data\n",
    "X = listings_df['review_scores_rating']\n",
    "y = listings_df['legal_listing']\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = sm.Logit(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "\n",
    "print(result.summary())\n",
    "\n",
    "# Perform the Wald test\n",
    "wald_test = result.wald_test(\"review_scores_rating = 0\", scalar=True)\n",
    "\n",
    "# Print the Wald test results\n",
    "print(\"\\n       Wald Test Results\")\n",
    "print(\"Test Statistic:\", wald_test.statistic)\n",
    "print(\"P-value:\", wald_test.pvalue)\n",
    "print(\"Degrees of Freedom:\", wald_test.df_denom)\n",
    "\n",
    "# Create a new row and append it in the variables_significance_info.csv\n",
    "\"\"\"\n",
    "variables_significance_info = pd.read_csv(os.path.join('data', 'variables_sigificance_info.csv'))\n",
    "new_row = ['review_scores_rating', 4.772477793182879e-07, True]\n",
    "\n",
    "variables_significance_info = pd.concat(new_row, ignore_index=True)\n",
    "\n",
    "variables_significance_info.to_csv(os.path.join('data', 'variables_sigificance_info.csv'), index=False)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Price**\n",
    "\n",
    "**P-value** = [1.3378417181893986e-12]      [<0.05]\n",
    "\n",
    "**Selection** =[True/False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.616955\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:          legal_listing   No. Observations:                 6695\n",
      "Model:                          Logit   Df Residuals:                     6693\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Fri, 24 Nov 2023   Pseudo R-squ.:                0.007930\n",
      "Time:                        13:58:34   Log-Likelihood:                -4130.5\n",
      "converged:                       True   LL-Null:                       -4163.5\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.431e-16\n",
      "=================================================================================\n",
      "                    coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "const             0.5094      0.045     11.222      0.000       0.420       0.598\n",
      "price_cleaned     0.0012      0.000      7.090      0.000       0.001       0.002\n",
      "=================================================================================\n",
      "\n",
      "       Wald Test Results\n",
      "Test Statistic: 50.272902792802135\n",
      "P-value: 1.3378417181893986e-12\n",
      "Degrees of Freedom: 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using the logistic regression Wald test, see if there is a connection between price (independent variable)\n",
    "and legal_listing (dependent variable). The null hypothesis is that there is no connection between the two variables.\n",
    "price is a continuous variable from 0 to infinity, while legal_listing is a categorical boolean variable.\n",
    "\"\"\"\n",
    "# Load cleaned data\n",
    "listings_df = pd.read_csv(os.path.join('data', 'yvr_listing_data_cleaned.csv'))\n",
    "\n",
    "# Remove rows where price is NaN\n",
    "listings_df = listings_df[listings_df['price'].notna()]\n",
    "\n",
    "# Prepare the data\n",
    "listings_df['price_cleaned']=listings_df[\"price\"].str.replace('[\\$,]', '', regex=True).astype(float)\n",
    "X = listings_df['price_cleaned']\n",
    "y = listings_df['legal_listing']\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model = sm.Logit(y, sm.add_constant(X))\n",
    "result = model.fit()\n",
    "\n",
    "print(result.summary())\n",
    "\n",
    "# Perform the Wald test\n",
    "wald_test = result.wald_test(\"price_cleaned = 0\", scalar=True)\n",
    "\n",
    "# Print the Wald test results\n",
    "print(\"\\n       Wald Test Results\")\n",
    "print(\"Test Statistic:\", wald_test.statistic)\n",
    "print(\"P-value:\", wald_test.pvalue)\n",
    "print(\"Degrees of Freedom:\", wald_test.df_denom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Host_is_superhost**\n",
    "\n",
    "**P-value** = [7.521053153927296e-32]      [<0.05]\n",
    "\n",
    "**Selection** =[True/False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square statistic: 137.93740541419484\n",
      "P-value: 7.521053153927296e-32\n",
      "Degrees of Freedom: 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using the chi-square test, see if there is a connection between host_is_superhost (independent variable)\n",
    "and legal_listing (dependent variable). The null hypothesis is that there is no connection between the two variables.\n",
    "host_is_superhost variable is a categorical boolean variable, while legal_listing is also a categorical boolean variable.\n",
    "\"\"\"\n",
    "\n",
    "# Load cleaned data\n",
    "listings_df = pd.read_csv(os.path.join('data', 'yvr_listing_data_cleaned.csv'))\n",
    "\n",
    "# Remove rows where host_is_superhost is NaN\n",
    "listings_df = listings_df[listings_df['host_is_superhost'].notna()]\n",
    "\n",
    "# construct the contingency table\n",
    "contingency_table = pd.crosstab(listings_df['host_is_superhost'], listings_df['legal_listing'])\n",
    "\n",
    "# operate the chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the chi-square test results\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"P-value:\", p)\n",
    "print(\"Degrees of Freedom:\", dof)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**room_type**\n",
    "\n",
    "**P-value** = [0.00032175629100491637]      [<0.05]\n",
    "\n",
    "**Selection** =[True/False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square statistic: 18.65777655185931\n",
      "P-value: 0.00032175629100491637\n",
      "Degrees of Freedom: 3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using the chi-square test, see if there is a connection between room_type (independent variable)\n",
    "and legal_listing (dependent variable). The null hypothesis is that there is no connection between the two variables.\n",
    "room_type variable is a categorical variable, while legal_listing is a categorical boolean variable.\n",
    "\"\"\"\n",
    "\n",
    "# Load cleaned data\n",
    "listings_df = pd.read_csv(os.path.join('data', 'yvr_listing_data_cleaned.csv'))\n",
    "\n",
    "# Remove rows where host_is_superhost is NaN\n",
    "listings_df = listings_df[listings_df['room_type'].notna()]\n",
    "\n",
    "# construct the contingency table\n",
    "contingency_table = pd.crosstab(listings_df['room_type'], listings_df['legal_listing'])\n",
    "\n",
    "# operate the chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the chi-square test results\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"P-value:\", p)\n",
    "print(\"Degrees of Freedom:\", dof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**property_type**\n",
    "\n",
    "**P-value** = [4.202727980331047e-117]      [<0.05]\n",
    "\n",
    "**Selection** =[True/False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square statistic: 713.9546041312349\n",
      "P-value: 4.202727980331047e-117\n",
      "Degrees of Freedom: 52\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using the chi-square test, see if there is a connection between property_type (independent variable)\n",
    "and legal_listing (dependent variable). The null hypothesis is that there is no connection between the two variables.\n",
    "property_type variable is a categorical variable, while legal_listing is a categorical boolean variable.\n",
    "\"\"\"\n",
    "\n",
    "# Load cleaned data\n",
    "listings_df = pd.read_csv(os.path.join('data', 'yvr_listing_data_cleaned.csv'))\n",
    "\n",
    "# Remove rows where host_is_superhost is NaN\n",
    "listings_df = listings_df[listings_df['property_type'].notna()]\n",
    "\n",
    "# construct the contingency table\n",
    "contingency_table = pd.crosstab(listings_df['property_type'], listings_df['legal_listing'])\n",
    "\n",
    "# operate the chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the chi-square test results\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"P-value:\", p)\n",
    "print(\"Degrees of Freedom:\", dof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**neighbourhood_cleansed**\n",
    "\n",
    "**P-value** = [2.68494430792469e-84]      [<0.05]\n",
    "\n",
    "**Selection** =[True/False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square statistic: 463.65793201510144\n",
      "P-value: 2.68494430792469e-84\n",
      "Degrees of Freedom: 22\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using the chi-square test, see if there is a connection between neighbourhood_cleansed (independent variable)\n",
    "and legal_listing (dependent variable). The null hypothesis is that there is no connection between the two variables.\n",
    "neighbourhood_cleansed variable is a categorical variable, while legal_listing is a categorical boolean variable.\n",
    "\"\"\"\n",
    "\n",
    "# Load cleaned data\n",
    "listings_df = pd.read_csv(os.path.join('data', 'yvr_listing_data_cleaned.csv'))\n",
    "\n",
    "# Remove rows where host_is_superhost is NaN\n",
    "listings_df = listings_df[listings_df['neighbourhood_cleansed'].notna()]\n",
    "\n",
    "# construct the contingency table\n",
    "contingency_table = pd.crosstab(listings_df['neighbourhood_cleansed'], listings_df['legal_listing'])\n",
    "\n",
    "# operate the chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Print the chi-square test results\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "print(\"P-value:\", p)\n",
    "print(\"Degrees of Freedom:\", dof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**continuing...**\n",
    "\n",
    "**P-value** = [2.68494430792469e-84]      [<0.05]\n",
    "\n",
    "**Selection** =[True/False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
